{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily create an AIConfig from existing Openai code\n",
    "1. Basic usage\n",
    "2. Load existing aiconfig and continue\n",
    "3. Capture function calling\n",
    "4. Use Client API\n",
    "5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ~/.env file with this line: export OPENAI_API_KEY=<your key here>\n",
    "# You can get your key from https://platform.openai.com/api-keys \n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_parsers\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "\n",
    "output_path = \"my-first-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"her what's up\")\n",
    "    print(f\"{response=}\")\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-21 23:20:06,867 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1703218614711964000\n",
      "[INFO] 2023-12-21 23:20:06,869 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1703218614711964000\n",
      "[INFO] 2023-12-21 23:20:06,870 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1703218614711964000\n",
      "[INFO] 2023-12-21 23:20:06,871 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell me a joke about apples', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1703218614711964000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her what's up\n",
      "response=ChatCompletion(id='chatcmpl-8YRJ0qPOK9lNRMtayINvWGElc5zgP', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Why did the apple go to the doctor?\\n\\nBecause it wasn't peeling well!\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1703218806, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=13, total_tokens=30))\n",
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell me a joke about apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json now represents your existing app:\n",
      "\n",
      "{\n",
      "  \"name\": \"\",\n",
      "  \"schema_version\": \"latest\",\n",
      "  \"metadata\": {\n",
      "    \"parameters\": {},\n",
      "    \"models\": {}\n",
      "  },\n",
      "  \"description\": \"\",\n",
      "  \"prompts\": [\n",
      "    {\n",
      "      \"name\": \"prompt_0\",\n",
      "      \"input\": \"Tell me a joke about apples\",\n",
      "      \"metadata\": {\n",
      "        \"model\": {\n",
      "          \"name\": \"gpt-3.5-turbo\",\n",
      "          \"settings\": {\n",
      "            \"model\": \"gpt-3.5-turbo\",\n",
      "            \"top_p\": 1,\n",
      "            \"max_tokens\": 3000,\n",
      "            \"temperature\": 1,\n",
      "            \"stream\": false\n",
      "          }\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"remember_chat_context\": true\n",
      "      },\n",
      "      \"outputs\": [\n",
      "        {\n",
      "          \"output_type\": \"execute_result\",\n",
      "          \"execution_count\": 0,\n",
      "          \"data\": {\n",
      "            \"content\": \"Why did the apple go to the doctor?\\n\\nBecause it wasn't peeling well!\",\n",
      "            \"role\": \"assistant\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"id\": \"chatcmpl-8YRJ0qPOK9lNRMtayINvWGElc5zgP\",\n",
      "            \"created\": 1703218806,\n",
      "            \"model\": \"gpt-3.5-turbo-0613\",\n",
      "            \"object\": \"chat.completion\",\n",
      "            \"usage\": {\n",
      "              \"completion_tokens\": 17,\n",
      "              \"prompt_tokens\": 13,\n",
      "              \"total_tokens\": 30\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"my-first-aiconfig.json now represents your existing app:\\n\")\n",
    "result = json.load(open(output_path))\n",
    "print(\n",
    "    json.dumps(result, indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom in on the prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple go to the doctor?\\n\\nBecause it wasn't peeling well!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8YRJ0qPOK9lNRMtayINvWGElc5zgP\",\n",
      "        \"created\": 1703218806,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 17,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 30\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"Zoom in on the prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(result[\"prompts\"][0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continue from existing aiconfig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-21 23:20:37,611 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1703218614711964000\n",
      "[INFO] 2023-12-21 23:20:37,614 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Tell a joke about apples in Shakespearian English.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1703218614711964000\n",
      "[INFO] 2023-12-21 23:20:37,615 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1703218614711964000\n",
      "[INFO] 2023-12-21 23:20:37,616 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Tell a joke about apples in Shakespearian English.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1703218614711964000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her what's up\n",
      "response=ChatCompletion(id='chatcmpl-8YRJU4XQpmqhNxb509bsJJWog8JEE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Why did the apple become a thespian?\\n\\nBecause it wanted to play a part in Macbeth, but it couldn\\'t resist saying, \"Is this a dagger I spy, or art thou just peeler?\"', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1703218836, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=43, prompt_tokens=19, total_tokens=62))\n",
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell a joke about apples in Shakespearian English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-first-aiconfig.json has your existing prompts:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Tell me a joke about apples\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did the apple go to the doctor?\\n\\nBecause it wasn't peeling well!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8YRJ0qPOK9lNRMtayINvWGElc5zgP\",\n",
      "        \"created\": 1703218806,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 17,\n",
      "          \"prompt_tokens\": 13,\n",
      "          \"total_tokens\": 30\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"my-first-aiconfig.json has your existing prompts:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And your new prompt:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_1\",\n",
      "  \"input\": \"Tell a joke about apples in Shakespearian English.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Why did Romeo have an apple tree in his garden?\\n\\nForsooth, he wished to have a fruit with his Juliet, but verily, he missed the peach and ended up with a Granny Smith!\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8R35bYMBANDKFbmTP0vN2y4K4p9j5\",\n",
      "        \"created\": 1701457423,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 41,\n",
      "          \"prompt_tokens\": 19,\n",
      "          \"total_tokens\": 60\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"And your new prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(prompts[1], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Capture function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My existing app using function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call Capture\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "output_path = \"my-function-calling-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str) -> dict[str, Any]:\n",
    "    return { \"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\" }\n",
    "\n",
    "\n",
    "def run_my_existing_weather_function_calling_app():\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}],\n",
    "        \"functions\": [\n",
    "            {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "\n",
    "    function_call_response = get_current_weather(location=\"Boston\", unit=\"celsius\")\n",
    "    print(response)\n",
    "\n",
    "    completion_params = {\n",
    "      \"model\": \"gpt-3.5-turbo-0613\",\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "        {\"role\": \"assistant\", \"content\": 'null', \"function_call\": {\n",
    "              \"name\": \"get_current_weather\",\n",
    "              \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
    "            }},\n",
    "        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": str(function_call_response)}\n",
    "\n",
    "      ],\n",
    "      \"functions\": [\n",
    "        {\n",
    "          \"name\": \"get_current_weather\",\n",
    "          \"description\": \"Get the current weather in a given location\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "              },\n",
    "              \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:45,823 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:45,825 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:45,827 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:45,828 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8R35d8LM16rpKZPjstq56h2FqbyoM', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1701457425, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=82, total_tokens=100))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:46,462 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:46,463 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo-0613', 'messages': [{'role': 'user', 'content': 'What is the weather like in Boston?'}, {'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': \"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"}], 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:46,465 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data={'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, mime_type=None, metadata={})]), Prompt(name='prompt', input=PromptInput(data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:46,466 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt_1', input='What is the weather like in Boston?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[ExecuteResult(output_type='execute_result', execution_count=None, data={'role': 'assistant', 'content': 'null', 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}}, mime_type=None, metadata={})]), Prompt(name='prompt', input=PromptInput(data=None, role='function', name='get_current_weather', content=\"{'temperature': 22, 'unit': 'celsius', 'description': 'Sunny'}\"), metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo-0613', settings={'model': 'gpt-3.5-turbo-0613', 'functions': [{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_weather_function_calling_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-function-calling-aiconfig.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"What is the weather like in Boston?\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo-0613\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"functions\": [\n",
      "          {\n",
      "            \"name\": \"get_current_weather\",\n",
      "            \"description\": \"Get the current weather in a given location\",\n",
      "            \"parameters\": {\n",
      "              \"type\": \"object\",\n",
      "              \"properties\": {\n",
      "                \"location\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "                },\n",
      "                \"unit\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"enum\": [\n",
      "                    \"celsius\",\n",
      "                    \"fahrenheit\"\n",
      "                  ]\n",
      "                }\n",
      "              },\n",
      "              \"required\": [\n",
      "                \"location\"\n",
      "              ]\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"data\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"null\",\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"metadata\": {}\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspect my-function-calling-aiconfig.json:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai_client\n",
    "\n",
    "\n",
    "output_path = \"my-aiconfig-from-Client-API.json\"\n",
    "\n",
    "client = get_completion_create_wrapped_openai_client(output_path)\n",
    "\n",
    "def run_my_existing_client_api_app():\n",
    "    completion_params = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"stream\": False,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"content\": \"Compare and contrast bananas and cucumbers.\",\n",
    "                        \"role\": \"user\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "    response = client.chat.completions.create(**completion_params)\n",
    "    print(type(response))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-01 14:03:56,300 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:56,302 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Compare and contrast bananas and cucumbers.', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:56,303 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n",
      "[INFO] 2023-12-01 14:03:56,304 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Compare and contrast bananas and cucumbers.', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1701457422515626000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_client_api_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect my-aiconfig-from-Client-API.json:\n",
      "\n",
      "{\n",
      "  \"name\": \"prompt_0\",\n",
      "  \"input\": \"Compare and contrast bananas and cucumbers.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt-3.5-turbo\",\n",
      "      \"settings\": {\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"top_p\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 1,\n",
      "        \"stream\": false\n",
      "      }\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"remember_chat_context\": true\n",
      "  },\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"output_type\": \"execute_result\",\n",
      "      \"execution_count\": 0,\n",
      "      \"data\": {\n",
      "        \"content\": \"Bananas and cucumbers are both popular fruits that are consumed worldwide. However, they have several contrasting characteristics as well.\\n\\nPhysical Appearance:\\n- Bananas: They are elongated fruits with a curved shape, typically yellow in color when ripe, and have a thick, peeling skin.\\n- Cucumbers: They are cylindrical in shape, usually longer than bananas, and have a smooth, dark green skin.\\n\\nTaste and Texture:\\n- Bananas: They have a sweet and creamy taste with a soft, mushy texture when ripe, making them easy to chew and digest.\\n- Cucumbers: They have a refreshingly mild and slightly bitter taste with a crispy, crunchy texture. They are known for their high water content.\\n\\nNutritional Profile:\\n- Bananas: They are a good source of potassium, vitamin C, vitamin B6, and dietary fiber. They also contain natural sugars, making them a good energy source.\\n- Cucumbers: They are low in calories and rich in water, making them hydrating. They also provide some vitamin K, vitamin C, and dietary fiber, although in smaller amounts compared to bananas.\\n\\nCulinary Uses:\\n- Bananas: They are consumed as a snack, added to smoothies, used in baking (for desserts like banana bread), and can be frozen for later use. They are also commonly used in making ice cream and milkshakes.\\n- Cucumbers: They are often used fresh in salads, sandwiches, and pickles. Cucumbers can also be blended into juices or added to infused water for flavor.\\n\\nCultural Significance:\\n- Bananas: They are staple fruits in many tropical countries and are an important part of various cuisines. Bananas are widely associated with health benefits, energy, and are a popular breakfast choice.\\n- Cucumbers: They are extensively used in salads and pickling traditions in cultures around the world, adding a refreshing and crunchy element to meals.\\n\\nOverall, while bananas and cucumbers share some similarities as fruits, their distinctive characteristics, taste, and culinary uses set them apart from each other.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"id\": \"chatcmpl-8R35ecL6ekpb6Ca5fBxOtiyYplORM\",\n",
      "        \"created\": 1701457426,\n",
      "        \"model\": \"gpt-3.5-turbo-0613\",\n",
      "        \"object\": \"chat.completion\",\n",
      "        \"usage\": {\n",
      "          \"completion_tokens\": 430,\n",
      "          \"prompt_tokens\": 16,\n",
      "          \"total_tokens\": 446\n",
      "        },\n",
      "        \"finish_reason\": \"stop\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(f\"Inspect {output_path}:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "from aiconfig.Config import AIConfigRuntime\n",
    "\n",
    "existing_aiconfig = AIConfigRuntime.create()\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=existing_aiconfig,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-22 00:06:07,549 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.Config' data={'gpt-3.5-turbo': 'gpt-3.5-turbo', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'prompt_name': 'prompt', 'params': None} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:07,553 callback.py:140: Callback called. event\n",
      ": name='on_serialize_start' file='aiconfig.default_parsers.openai' data={'prompt_name': 'prompt', 'data': {'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}, 'parameters': {}, 'kwargs': {}} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:07,554 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.default_parsers.openai' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:07,555 callback.py:140: Callback called. event\n",
      ": name='on_serialize_complete' file='aiconfig.Config' data={'result': [Prompt(name='prompt', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[])]} ts_ns=1703221540962554000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response: \n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
     ]
    }
   ],
   "source": [
    "run_my_existing_openai_app(\"Are tomatoes fruits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-12-22 00:06:18,175 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'prompt_0', 'params': None, 'options': None, 'kwargs': {}} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:18,177 callback.py:140: Callback called. event\n",
      ": name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {}} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:18,178 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='prompt_0', input='Are tomatoes fruits?', metadata=PromptMetadata(model=ModelMetadata(name='gpt-3.5-turbo', settings={'model': 'gpt-3.5-turbo', 'top_p': 1, 'max_tokens': 3000, 'temperature': 1, 'stream': False}), tags=None, parameters={}, remember_chat_context=True), outputs=[]), 'params': {}} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:18,180 callback.py:140: Callback called. event\n",
      ": name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 1, 'top_p': 1, 'stream': False, 'model': 'gpt-3.5-turbo', 'max_tokens': 3000, 'messages': [{'content': 'Are tomatoes fruits?', 'role': 'user'}]}} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:19,423 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'Yes, tomatoes are fruits. Although they are commonly referred to as vegetables, tomatoes are botanically classified as fruits because they develop from the ovary of a flowering plant and contain seeds.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8YS1i6Nsf0kex8cb0tIQOr8yixtQO', 'created': 1703221578, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 38, 'prompt_tokens': 11, 'total_tokens': 49}, 'finish_reason': 'stop'})]} ts_ns=1703221540962554000\n",
      "[INFO] 2023-12-22 00:06:19,424 callback.py:140: Callback called. event\n",
      ": name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'Yes, tomatoes are fruits. Although they are commonly referred to as vegetables, tomatoes are botanically classified as fruits because they develop from the ovary of a flowering plant and contain seeds.', 'role': 'assistant'}, mime_type=None, metadata={'id': 'chatcmpl-8YS1i6Nsf0kex8cb0tIQOr8yixtQO', 'created': 1703221578, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 38, 'prompt_tokens': 11, 'total_tokens': 49}, 'finish_reason': 'stop'})]} ts_ns=1703221540962554000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "Yes, tomatoes are fruits. Although they are commonly referred to as vegetables, tomatoes are botanically classified as fruits because they develop from the ovary of a flowering plant and contain seeds.\n",
      "len(outputs)=1\n",
      "output={'content': 'Yes, tomatoes are fruits. Although they are commonly referred to as vegetables, tomatoes are botanically classified as fruits because they develop from the ovary of a flowering plant and contain seeds.', 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "await existing_aiconfig.run(\"prompt_0\")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(existing_aiconfig.get_output_text(\"prompt_0\"))\n",
    "outputs = existing_aiconfig.get_prompt(\"prompt_0\").outputs\n",
    "print(f\"{len(outputs)=}\")\n",
    "output = outputs[0].data\n",
    "print(f\"{output=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
